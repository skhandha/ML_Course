{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBcLbp-UR0hZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning Methods Introduction\n",
        "Supervised learning\n",
        "Group of predictors (classifiers / regressors)\n"
      ],
      "metadata": {
        "id": "VP8auTLwSFjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Methods:\n",
        "**Bootstrap Aggregrating or Bagging**\n",
        "- Scikit-learn reference\n",
        "- Bootstrap sampling: Sampling with replacement\n",
        "- Combine by averaging the output (regression)\n",
        "- Combine by voting (classification)\n",
        "- Can be applied to many classifiers which includes ANN, Cart, etc.\n",
        "\n",
        "**Pasting**\n",
        "- Sampling without replacement\n",
        "**Boosting**\n",
        "- Train week classifiers\n",
        "- Add them to a final strong classifier by weighting. Weighting by accuracy (typically)\n",
        "- Once added, the data are reweighted\n",
        "  - Missclassified samples gain weight\n",
        "  - Correctly classified samples lose weight (Exception: Boost by majority and BrownBoost - decrease the weight of repeatedly misclassified examples).\n",
        "  - Algo are forced to learn more from misclassified samples\n",
        "**Stacking**\n",
        "- Also known as Stacked generalization\n",
        "- From Kaggle: Combine information from multiple predictive models to generate a new model. Often times the stacked model (also called 2nd-level model) will outperform each of the individual models due to its smoothing nature and ability to highlight each base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different.\n",
        "- Training a learning algorithm to combine the predictions of several other learning algorithms\n",
        "  - Step 1: Train learning algo\n",
        "  - Step 2: Combiner algo is trained using algo predictions from step 1\n"
      ],
      "metadata": {
        "id": "1yF3_DyoSQQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Ensemble Methods\n",
        "- Bayes Optimal Classifier\n",
        "  - An ensemble of all the hypotheses in the hypothesis space.\n",
        "  - Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a sampled from a system if that hypothesis were true.\n",
        "  - To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis\n",
        "- Bayesian Parameter Averaging\n",
        "  - An ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypothesis from the hypothesis space, and combining them using Bayes' law\n",
        "  - Unlik the Bayes optimal classifier, Bayesian model averaging (BMA) can be practically implemented\n",
        "  - Hypothesis are typically sampled using a Monte Carlo sampling technique such as MCMC.\n",
        "- Bayesian Model Combination\n",
        "  - Instead of Sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters)\n",
        "  - This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model\n",
        "  - Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging.\n",
        "- Buckets of Models\n",
        "  - An ensemble technique in which a model selection algorithm is used to choose the best model for each problem\n",
        "  - When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.\n",
        "\n",
        "R released:\n",
        "- BMS (Bayesian Model Selection) package\n",
        "- BAS( Bayesian Apaptive Sampling) package\n",
        "- BMA package\n",
        "\n",
        "Note: Ensemble Methods:\n",
        "- work best with independent predictors"
      ],
      "metadata": {
        "id": "3KrhkH1vUN6o"
      }
    }
  ]
}